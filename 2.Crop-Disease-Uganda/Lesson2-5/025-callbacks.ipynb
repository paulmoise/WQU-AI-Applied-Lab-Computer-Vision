{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<p>\n",
    "  <b>AI Lab: Deep Learning for Computer Vision</b><br>\n",
    "  <b><a href=\"https://www.wqu.edu/\">WorldQuant University</a></b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <p>\n",
    "    <center><b>Usage Guidelines</b></center>\n",
    "  </p>\n",
    "  <p>\n",
    "    This file is licensed under <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International</a>.\n",
    "  </p>\n",
    "  <p>\n",
    "    You <b>can</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">✓</span> Download this file</li>\n",
    "      <li><span style=\"color: green\">✓</span> Post this file in public repositories</li>\n",
    "    </ul>\n",
    "    You <b>must always</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">✓</span> Give credit to <a href=\"https://www.wqu.edu/\">WorldQuant University</a> for the creation of this file</li>\n",
    "      <li><span style=\"color: green\">✓</span> Provide a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">link to the license</a></li>\n",
    "    </ul>\n",
    "    You <b>cannot</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: red\">✗</span> Create derivatives or adaptations of this file</li>\n",
    "      <li><span style=\"color: red\">✗</span> Use this file for commercial purposes</li>\n",
    "    </ul>\n",
    "  </p>\n",
    "  <p>\n",
    "    Failure to follow these guidelines is a violation of your terms of service and could lead to your expulsion from WorldQuant University and the revocation your certificate.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the model, let's set up our environment and prepare the data. We'll first load the necessary libraries and print out library versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchinfo\n",
    "import torchvision\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"torch version : \", torch.__version__)\n",
    "print(\"torchvision version : \", torchvision.__version__)\n",
    "print(\"torchinfo version : \", torchinfo.__version__)\n",
    "print(\"numpy version : \", np.__version__)\n",
    "print(\"matplotlib version : \", matplotlib.__version__)\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check if GPUs are available and set our device accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in the `data_p2` directory within which is the `data_undersampled` directory. In that folder we have the `train` subdirectory that contains the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.1:** Assign `data_dir` the path to the training data using `os.path.join`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"data_p2\", \"data_undersampled\", \"train\")\n",
    "\n",
    "print(\"Data Directory:\", data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may remember that images from each class are contained in separate subdirectories within `data_dir`, where the name of each subdirectory is the name of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.2:** Create a list of class names in this data using `os.listdir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(data_dir)\n",
    "\n",
    "print(\"List of classes:\", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous lessons, we'll standardize the images using the following set of transformations:\n",
    "\n",
    "- Convert any grayscale images to RGB format with a custom class\n",
    "- Resize the image, so that they're all the same size (we chose $224$ x $224$)\n",
    "- Convert the image to a Tensor of pixel values\n",
    "- Normalize the data\n",
    "\n",
    "Here's the custom transformation that we've used before which converts images to RGB format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToRGB(object):\n",
    "    def __call__(self, img):\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the transformation pipeline. In the normalization step, use the `mean` and `std` values from our previous lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.3:** Create the transformation pipeline using `transforms.Compose` from `torchvision` package. Follow what we did in the previous lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_normalized = transforms.Compose([\n",
    "    ConvertToRGB(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "print(type(transform_normalized))\n",
    "print(\"----------------\")\n",
    "print(transform_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the dataset and apply our transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.4:** Make a dataset using `ImageFolder` from `datasets` and make sure to apply `transform_normalized` transformation pipeline. Then print the length of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataset = datasets.ImageFolder(\n",
    "    root=data_dir,\n",
    "    transform=transform_normalized,\n",
    ")\n",
    "\n",
    "\n",
    "print('Length of dataset:', len(normalized_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a train/validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we'll divide our data into two parts. One part is for training the model, the other part is for evaluating it on unseen images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.5:** Use `random_split` to create a 80/20 split (training dataset should have 80% of the data, validation dataset should have 20% of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <p><b>About random number generators</b></p>\n",
    "<p>The following cell adds a <code>generator=g</code> line of code that is not present in the video. This is something we have added to make sure you always get the same results in your predictions. Please don't change it or remove it.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important, don't change this!\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    normalized_dataset,\n",
    "    [0.8,  0.2],\n",
    "    generator=g,\n",
    ")\n",
    "\n",
    "print(\"Length of training dataset:\", len(train_dataset))\n",
    "print(\"Length of validation dataset:\", len(val_dataset))\n",
    "\n",
    "percent_train = np.round(100 * len(train_dataset) / len(normalized_dataset), 2)\n",
    "percent_val = np.round(100 * len(val_dataset) / len(normalized_dataset), 2)\n",
    "\n",
    "print(f\"Train data is {percent_train}% of full data\")\n",
    "print(f\"Validation data is {percent_val}% of full data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again check how the observations are distributed across classes. We'll reuse the `class_counts` function from `training.py` that we used previously. We want to check the distribution of the training and the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.6:** Use `class_counts` function on the `train_dataset` and visualize the results with a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import class_counts\n",
    "\n",
    "train_counts = class_counts(train_dataset)\n",
    "# Make a bar chart from the function output\n",
    "train_counts.sort_values().plot(kind=\"bar\")\n",
    "# Add axis labels and title\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Frequency [count]\")\n",
    "plt.title(\"Distribution of Classes in Training Dataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.7:** Use the `class_counts` function on the validation split. Make sure to again visualize the results with a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = class_counts(val_dataset)\n",
    "\n",
    "# Make a bar chart from the function output\n",
    "val_counts.sort_values().plot(kind=\"bar\")\n",
    "# Add axis labels and title\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Frequency [count]\")\n",
    "plt.title(\"Distribution of Classes in Validation Dataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create `DataLoader` objects. We'll use a batch size of 32 and create one `DataLoader` for training and another for validation data. Remember that in training we want to shuffle the data after each epoch and in validation we don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.8:** Create the training loader (with shuffling on) and the validation loader (with shuffling off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(type(train_loader))\n",
    "print(type(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection for Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're using Transfer Learning, choosing the right pre-trained model is crucial. We'll select the same model as in the previous lesson. This model has been trained on a large and diverse dataset, ensuring it has learned features that are broadly applicable to various tasks, including ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.9:** Define a `resnet50` model in the same way we defined it in the previous lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Transfer Learning, we don't want to tweak the model weights when we train the model on our data. So let's make sure the weights are fixed! \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.10:** Fix the parameters of the model such that they'll not be updated once we train the model on our task. Remember how we did this in the previous lesson?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the models weights\n",
    "for params in model.parameters():\n",
    "    params.required_grad = False\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as we did before, we now want to change the output layer of the model (layer `model.fc`). We want to replace it with a dense layer and an output layer. \n",
    "\n",
    "But we need to know how many features will be going into the dense layer that we want to add. This means we should first compute the number of features going into the last layer of the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.11:** Compute the number of features going into the last layer of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feat = model.fc.in_features\n",
    "\n",
    "print(in_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Now we can change the last layer (layer `model.fc`). We want to change it to:\n",
    "- a dense layer with 256 neurons\n",
    "- followed by ReLU activation\n",
    "- then add `p=0.5` of Dropout\n",
    "- followed by the output layer with 5 neurons (because our data has 5 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.12:** Fill in the missing parts of code below that changes the last layer of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_last_layer = nn.Sequential()\n",
    "\n",
    "modified_last_layer.append(nn.Linear(in_feat, 256))\n",
    "\n",
    "relu = nn.ReLU()\n",
    "modified_last_layer.append(relu)\n",
    "\n",
    "modified_last_layer.append(nn.Dropout(p=0.5))\n",
    "\n",
    "linear = nn.Linear(256, len(classes))\n",
    "modified_last_layer.append(linear)\n",
    "\n",
    "model.fc = modified_last_layer\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the last layer of the model is now different and exactly what we wanted.\n",
    "\n",
    "We're ready to start fitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, before we start training, we need to define the loss and what optimizer we'll use. For loss function we'll go with cross entropy. For the optimizer we'll choose the Adam optimizer as we've done before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.13:** Define cross-entropy as the loss function and set Adam optimizer to be the optimizer. You can use the default learning rate and `weight_decay=1e-4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=0.0001)\n",
    "print(loss_fn)\n",
    "print(\"----------------------\")\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's make sure that we use the GPU that we have at our disposal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.14:** Place our model on `device`. The code we provided below prints out the device that the model is on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place model on device\n",
    "model.to(device)\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's get more information about our model by calling the `summary` function on the model. You may remember that we've seen this function before. This function requires us to pass in two things: the model itself and the size of input tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.15:** Complete the `input_size` tuple that we are passing to `summary` function in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 224\n",
    "width = 224\n",
    "\n",
    "summary(model, input_size=(batch_size, 3, width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among other things, the output of the summary displays the amount of trainable parameters the model has. And it has many! \n",
    "\n",
    "While we added Dropout, which helps with preventing overfitting, we'll go further and take another step to make sure we don't overfit. When we fit, we'll check model performance at every epoch and stop fitting when the model stops improving. This brings us to callbacks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model with Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training of our model, we can use various callbacks. Callbacks allow us to customize and control the training process in fine-grained ways. We'll implement three key callbacks:\n",
    "        \n",
    "- **Learning Rate Scheduling**: Adjusts the learning rate over time, which can lead to better model performance.\n",
    "- **Early Stopping**: Halts training when the model's performance stops improving, which prevents overfitting. We'll stop if validation loss doesn't improve for at least 5 epochs.\n",
    "- **Checkpointing**: Saves the model every time validation loss gets better than in the epoch prior. This allows us to recover the best model once training completes.\n",
    "\n",
    "In order to use these callbacks, we need to implement them and then update the `train` function. \n",
    "\n",
    "For the Learning Rate Scheduling, we'll use `StepLR` from `torch.optim`. The `StepLR` scheduler decays the learning rate by multiplicative factor `gamma` every `step_size` epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.16:** Set `step_size` to $4$ and `gamma` factor to $0.2$. The rest of the code creates a `StepLR` Learning Rate Scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Period of learning rate decay\n",
    "step_size = 4\n",
    "# Multiplicative factor of learning rate decay\n",
    "gamma = 0.2\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "scheduler = StepLR(\n",
    "    optimizer,\n",
    "    step_size=step_size,\n",
    "    gamma=gamma,\n",
    ")\n",
    "\n",
    "print(type(scheduler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Early Stopping, we'll create a function `early_stopping` that we'll call from within the `train` function. The `early_stopping` function accepts:\n",
    "- the current validation loss,\n",
    "- the best validation loss so far\n",
    "- the number of epochs since validation loss last improved (counter).\n",
    "\n",
    "In the function we need to check if validation loss improved. If yes, we reset the counter. If not, we add one to the counter. We also need to check if validation loss hasn't improved in the last 5 epochs. If that is the case, we should set stopping to `True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.17:** Fill in the missing code in the definition of the `early_stopping` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(validation_loss, best_val_loss, counter):\n",
    "    \"\"\"Function that implements Early Stopping\"\"\"\n",
    "\n",
    "    stop = False\n",
    "\n",
    "    if validation_loss < best_val_loss:\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    # Check if counter is >= patience (5 epochs in our case)\n",
    "    # Set stop variable accordingly\n",
    "    if counter >= 5:\n",
    "        stop = True\n",
    "\n",
    "    return counter, stop\n",
    "\n",
    "\n",
    "early_stopping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a function that will take care of Checkpointing. In this function we need to check if validation loss improved. If yes, we save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpointing(validation_loss, best_val_loss, model, optimizer, save_path):\n",
    "\n",
    "    if validation_loss < best_val_loss:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": best_val_loss,\n",
    "            },\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"Checkpoint saved with validation loss {validation_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to modify the `train` function to include an option to use Callbacks. \n",
    "\n",
    "Notice that the modified `train` function below is quite similar to what we've used before. We just added `scheduler`, `checkpoint_path` and `early_stopping` as optional arguments. As you can see at the end of the modified `train` function, we use these three callbacks when function is called with appropriate inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import score, train_epoch\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=20,\n",
    "    device=\"cpu\",\n",
    "    scheduler=None,\n",
    "    checkpoint_path=None,\n",
    "    early_stopping=None,\n",
    "):\n",
    "    # Track the model progress over epochs\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    learning_rates = []\n",
    "\n",
    "    # Create the trackers if needed for checkpointing and early stopping\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    print(\"Model evaluation before start of training...\")\n",
    "    # Test on training set\n",
    "    train_loss, train_accuracy = score(model, train_loader, loss_fn, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    # Test on validation set\n",
    "    validation_loss, validation_accuracy = score(model, val_loader, loss_fn, device)\n",
    "    val_losses.append(validation_loss)\n",
    "    val_accuracies.append(validation_accuracy)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"\\n\")\n",
    "        print(f\"Starting epoch {epoch}/{epochs}\")\n",
    "\n",
    "        # Train one epoch\n",
    "        train_epoch(model, optimizer, loss_fn, train_loader, device)\n",
    "\n",
    "        # Evaluate training results\n",
    "        train_loss, train_accuracy = score(model, train_loader, loss_fn, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Test on validation set\n",
    "        validation_loss, validation_accuracy = score(model, val_loader, loss_fn, device)\n",
    "        val_losses.append(validation_loss)\n",
    "        val_accuracies.append(validation_accuracy)\n",
    "\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.4f}%\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print(f\"Validation accuracy: {validation_accuracy*100:.4f}%\")\n",
    "\n",
    "        # # Log the learning rate and have the scheduler adjust it\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        learning_rates.append(lr)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Checkpointing saves the model if current model is better than best so far\n",
    "        if checkpoint_path:\n",
    "            checkpointing(\n",
    "                validation_loss, best_val_loss, model, optimizer, checkpoint_path\n",
    "            )\n",
    "\n",
    "        # Early Stopping\n",
    "        if early_stopping:\n",
    "            early_stopping_counter, stop = early_stopping(\n",
    "                validation_loss, best_val_loss, early_stopping_counter\n",
    "            )\n",
    "            if stop:\n",
    "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "                break\n",
    "\n",
    "        if validation_loss < best_val_loss:\n",
    "            best_val_loss = validation_loss\n",
    "\n",
    "    return (\n",
    "        learning_rates,\n",
    "        train_losses,\n",
    "        val_losses,\n",
    "        train_accuracies,\n",
    "        val_accuracies,\n",
    "        epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model and callbacks ready, we'll proceed to train the model. During this phase, we'll observe how callbacks affect the training process and ultimately, the model's performance. \n",
    "\n",
    "Because we implemented early stopping, the model will stop training once its performance no longer improves. So we can set off to train for many epochs and training will stop when the model stops improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.18:** Define the number of training epochs to equal 50. The rest of the code provided below will call the `train` function and start the training. Note that this can take a while to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"> <strong>Regarding Model Training Times</strong>\n",
    "\n",
    "This task involves training the model for (at least) 50 epochs. This might take more than 60 minutes. Instead, we recommend you to skip the training process and load the pre-trained model that we have made available in the next few cells.\n",
    "\n",
    "<b>We strongly recommend you to use the saved model instead of training your own</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_to_train = 50\n",
    "\n",
    "train_results = train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=epochs_to_train,\n",
    "    device=device,\n",
    "    scheduler=scheduler,\n",
    "    checkpoint_path=\"model/LR_model.pth\",\n",
    "    early_stopping=early_stopping,\n",
    ")\n",
    "\n",
    "(\n",
    "    learning_rates,\n",
    "    train_losses,\n",
    "    valid_losses,\n",
    "    train_accuracies,\n",
    "    valid_accuracies,\n",
    "    epochs,\n",
    ") = train_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[RECOMMENDED]** Load the pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "model = torch.load(\"model_trained.pth\", weights_only=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the training indeed didn't go over all 50 epochs, but stopped earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training went on for {epochs} number of epochs before it stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Training Process and the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training's finished, we'll evaluate our model's performance and draw conclusions. We'll see how effectively our callbacks contributed to the training process and discuss the results. Let's first plot the learning curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_metrics_df = pd.read_csv(\"pretrained_model_evaluation_metrics.csv\")\n",
    "train_losses = eval_metrics_df['train_losses'].values\n",
    "valid_losses = eval_metrics_df['valid_losses'].values\n",
    "train_accuracies = eval_metrics_df['train_accuracies'].values\n",
    "valid_accuracies = eval_metrics_df['valid_accuracies'].values\n",
    "learning_rates = eval_metrics_df['learning_rates'].dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(valid_losses, label=\"Validation Loss\")\n",
    "plt.ylim([0, 1.7])\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.19:** Complete the code below to plot train and validation accuracies. You can follow what we did above for plotting train and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train accuracies, use label=\"Training Accuracy\"\n",
    "\n",
    "# Plot validation accuracies, use label=\"Validation Accuracy\"\n",
    "\n",
    "plt.ylim([0, 1])\n",
    "plt.title(\"Accuracy over epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the learning curve we see that overall training loss decreases and accuracy increases. Validation loss does not seem to improve that much beyond the first couple of epochs.\n",
    "\n",
    "Let's also inspect how the learning rate was changing during training due to the fact that we used a Learning Rate Scheduling Callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), learning_rates, marker=\"o\", label=\"Learning Rate\")\n",
    "plt.title(\"Learning Rate Schedule\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the learning rate decreases as our training progresses.\n",
    "\n",
    "Now it's time load the best model that we saved with checkpointing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model/LR_model.pth\")\n",
    "\n",
    "# Load the state dictionaries\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the confusion matrix for our model using the validation data, like we did in previous lessons.\n",
    "\n",
    "We'll obtain the probabilities that our model predicts by using the `predict` function from `training.py`. This function expects the model, the loader and the device as input arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.20:** Use the `predict` function from `training.py` to compute probabilities that our model predicts on the validation data. Then use `torch.argmax` and take these probabilities to compute the predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import predict\n",
    "\n",
    "probabilities_val = predict(model, val_loader, device)\n",
    "predictions_val = torch.argmax(probabilities_val, dim=1)\n",
    "E\n",
    "print(predictions_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get the target values and compute the confusion matrix. Again, same as we've done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_val = torch.cat([labels for _, labels in tqdm(val_loader, desc=\"Get Labels\")])\n",
    "\n",
    "cm = confusion_matrix(targets_val.cpu(), predictions_val.cpu())\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation=\"vertical\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, looks good! We're ready to use this model on our test set and prepare a CSV file that we can submit to the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission to Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition submission should contain predicted probabilities for each of the $5$ classes on a test set. So we'll need to run each test image through our model.\n",
    "\n",
    "Let's first find the test images. They are located in the `test` subdirectory within the `data_p2` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.21:** Assign `test_dir` the path to the test data using `os.path.join`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(\"data_p2\", \"test\")\n",
    "\n",
    "print(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition requires us to save the model predictions as a CSV file. The first column should be called ID and contains the image filename. The rest of the columns should be labeled by the class name.\n",
    "\n",
    "In order to get predicted probabilities of our model, we'll create a function `file_to_confidence` which is similar to what we created for this purpose in Project 1. The function makes model predictions on a single image. The steps in the function are:\n",
    "- Open the image.\n",
    "- Apply our transformation pipeline to the image as our model expects.\n",
    "- Use `unsqueeze` to change the image tensor to 4D ($1$ x $3$ x $224$ x $224$) as our model is expecting a batch of images.\n",
    "- Place image on device we're using.\n",
    "- Make prediction and pass it through a `SoftMax` to get probabilities (numbers between $0$ and $1$, that sum to $1$).\n",
    "- Convert result to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "\n",
    "def file_to_confidence(model, datadir, filename, transform_pipeline):\n",
    "    file_path = os.path.join(datadir, filename)\n",
    "    image = PIL.Image.open(file_path)\n",
    "    transformed = transform_pipeline(image)\n",
    "    unsqueezed = transformed.unsqueeze(0)\n",
    "    image_cuda = unsqueezed.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model_raw = model(image_cuda)\n",
    "        confidence = torch.nn.functional.softmax(model_raw, dim=1)\n",
    "\n",
    "    conf_df = pd.DataFrame([[filename] + confidence.tolist()[0]])\n",
    "    conf_df.columns = [\"ID\"] + train_dataset.dataset.classes\n",
    "\n",
    "    return conf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure this is working, let's call this function on a training image from the cassava mosaic disease class for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic_train_dir = os.path.join(\"data_p2\", \"train\", \"cassava-mosaic-disease-cmd\")\n",
    "mosaic_images = os.listdir(mosaic_train_dir)\n",
    "\n",
    "file_to_confidence(model, mosaic_train_dir, mosaic_images[0], transform_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks alright! The largest predicted probability on this mosaic image is for the mosaic disease class.\n",
    "\n",
    "Let's try one more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_confidence(model, mosaic_train_dir, mosaic_images[1], transform_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems in order. Now let's use `file_to_confidence` function on each test image to get the predictions for the competition submission. We can loop over the filenames and build up a list of DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5.22:** Fill in the missing code below and use `pd.concat` to assemble the list of DataFrames `small_dfs` into one big DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dfs = []\n",
    "\n",
    "for filename in tqdm(os.listdir(test_dir), desc=\"Predicting on test set\"):\n",
    "    small_dfs.append(\n",
    "        file_to_confidence(model, test_dir, filename, transform_normalized)\n",
    "    )\n",
    "\n",
    "confidence_df = pd.concat(small_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "confidence_df = confidence_df.sort_values(\"ID\").reset_index(drop=True)\n",
    "confidence_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the dataframe as a CSV in `submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! 🎉 We accomplished a lot in this notebook. Here are the key takeaways:\n",
    "\n",
    "- We used Transfer Learning to take a large existing model and specialize it to our competition.\n",
    "- We trained that model with the balanced dataset we created in an earlier lesson.\n",
    "- We implemented Callbacks using additional code in the training loop.\n",
    "- The Callbacks we implemented were: Learning Rate Scheduling, Checkpointing, and Early Stopping.\n",
    "- By reformatting the predictions of the model on the test set, we obtained a CSV file for competition submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "This file &#169; 2024 by [WorldQuant University](https://www.wqu.edu/) is licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
